{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InterpretabilityWorkbench Tutorial\n",
    "\n",
    "This tutorial walks through the complete workflow of the InterpretabilityWorkbench:\n",
    "1. Recording model activations\n",
    "2. Training sparse autoencoders (SAEs)\n",
    "3. Analyzing discovered features\n",
    "4. Creating live LoRA patches\n",
    "5. Evaluating the results\n",
    "\n",
    "**Expected runtime**: ~30 minutes (with GPU)\n",
    "**Requirements**: GPU with 8GB+ VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install -e .\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Local imports\n",
    "from trace import ActivationRecorder, FeatureAnalyzer\n",
    "from sae_train import train_sae, SparseAutoencoder\n",
    "from lora_patch import LoRAPatcher\n",
    "from eval import SAEEvaluator\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Record Model Activations\n",
    "\n",
    "We'll use a small model for this tutorial to keep runtime manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/DialoGPT-small\"  # Small model for tutorial\n",
    "LAYER_IDX = 8  # Middle layer\n",
    "MAX_SAMPLES = 1000  # Small dataset for tutorial\n",
    "ACTIVATION_FILE = \"tutorial_activations.parquet\"\n",
    "\n",
    "print(f\"Recording activations from {MODEL_NAME}, layer {LAYER_IDX}\")\n",
    "print(f\"Max samples: {MAX_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record activations\n",
    "recorder = ActivationRecorder(\n",
    "    model_name=MODEL_NAME,\n",
    "    layer_idx=LAYER_IDX,\n",
    "    output_path=ACTIVATION_FILE,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    max_length=256  # Shorter sequences for tutorial\n",
    ")\n",
    "\n",
    "# This will take a few minutes\n",
    "recorder.record(dataset_name=\"wikitext\")\n",
    "\n",
    "print(f\"\\nActivations saved to {ACTIVATION_FILE}\")\n",
    "file_size_mb = Path(ACTIVATION_FILE).stat().st_size / (1024*1024)\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the activation data\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table(ACTIVATION_FILE)\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check activation dimensions\n",
    "first_activation = df.iloc[0]['activation']\n",
    "print(f\"\\nActivation vector dimension: {len(first_activation)}\")\n",
    "print(f\"Sample activation values: {first_activation[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation statistics\n",
    "activations = np.array([act for act in df['activation']])\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "# Plot activation distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram of activation values\n",
    "axes[0].hist(activations.flatten(), bins=50, alpha=0.7)\n",
    "axes[0].set_title('Distribution of Activation Values')\n",
    "axes[0].set_xlabel('Activation Value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Mean activation per dimension\n",
    "mean_activations = np.mean(activations, axis=0)\n",
    "axes[1].plot(mean_activations)\n",
    "axes[1].set_title('Mean Activation per Dimension')\n",
    "axes[1].set_xlabel('Dimension')\n",
    "axes[1].set_ylabel('Mean Activation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Activation statistics:\")\n",
    "print(f\"  Mean: {np.mean(activations):.4f}\")\n",
    "print(f\"  Std: {np.std(activations):.4f}\")\n",
    "print(f\"  Min: {np.min(activations):.4f}\")\n",
    "print(f\"  Max: {np.max(activations):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Sparse Autoencoder\n",
    "\n",
    "Now we'll train an SAE to discover interpretable features in the recorded activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAE training configuration\n",
    "SAE_OUTPUT_DIR = \"tutorial_sae\"\n",
    "LATENT_DIM = 2048  # Expansion factor of ~2.7x (768 -> 2048 for DialoGPT-small)\n",
    "SPARSITY_COEF = 5e-4  # L1 penalty coefficient\n",
    "MAX_EPOCHS = 20  # Fewer epochs for tutorial\n",
    "\n",
    "print(f\"Training SAE with {LATENT_DIM} latent dimensions\")\n",
    "print(f\"Sparsity coefficient: {SPARSITY_COEF}\")\n",
    "print(f\"Max epochs: {MAX_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SAE\n",
    "sae_trainer = train_sae(\n",
    "    activation_path=ACTIVATION_FILE,\n",
    "    output_dir=SAE_OUTPUT_DIR,\n",
    "    layer_idx=LAYER_IDX,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    sparsity_coef=SPARSITY_COEF,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gpus=1 if torch.cuda.is_available() else 0\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"SAE saved to {SAE_OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained SAE and inspect its structure\n",
    "import safetensors.torch as safetensors\n",
    "import json\n",
    "\n",
    "sae_path = Path(SAE_OUTPUT_DIR) / f\"sae_layer_{LAYER_IDX}.safetensors\"\n",
    "metadata_path = Path(SAE_OUTPUT_DIR) / f\"sae_layer_{LAYER_IDX}_metadata.json\"\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"SAE Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load SAE weights\n",
    "sae_weights = safetensors.load_file(sae_path)\n",
    "print(f\"\\nSAE Weight shapes:\")\n",
    "for name, tensor in sae_weights.items():\n",
    "    print(f\"  {name}: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate SAE Performance\n",
    "\n",
    "Let's evaluate how well our SAE reconstructs the original activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the SAE\n",
    "evaluator = SAEEvaluator(\n",
    "    sae_path=str(sae_path),\n",
    "    activation_path=ACTIVATION_FILE,\n",
    "    layer_idx=LAYER_IDX\n",
    ")\n",
    "\n",
    "# Generate evaluation report\n",
    "report = evaluator.generate_report(\"tutorial_eval_report.json\")\n",
    "evaluator.print_summary(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SAE performance\n",
    "metrics = report['reconstruction_metrics']\n",
    "feature_analysis = report['feature_analysis']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Reconstruction loss\n",
    "axes[0, 0].bar(['Reconstruction Loss', 'Target (â‰¤0.15)'], \n",
    "               [metrics['reconstruction_loss'], 0.15],\n",
    "               color=['blue', 'red'])\n",
    "axes[0, 0].set_title('Reconstruction Loss vs Target')\n",
    "axes[0, 0].set_ylabel('MSE Loss')\n",
    "\n",
    "# Explained variance\n",
    "axes[0, 1].bar(['Explained Variance'], [metrics['explained_variance']], \n",
    "               color='green')\n",
    "axes[0, 1].set_title('Explained Variance (RÂ²)')\n",
    "axes[0, 1].set_ylabel('RÂ²')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Feature activity\n",
    "total_features = metrics['num_features']\n",
    "dead_features = feature_analysis['total_dead_features']\n",
    "active_features = total_features - dead_features\n",
    "\n",
    "axes[1, 0].pie([active_features, dead_features], \n",
    "               labels=['Active Features', 'Dead Features'],\n",
    "               autopct='%1.1f%%',\n",
    "               colors=['lightgreen', 'lightcoral'])\n",
    "axes[1, 0].set_title('Feature Activity Distribution')\n",
    "\n",
    "# Top active features\n",
    "if len(feature_analysis['most_active_features']) >= 5:\n",
    "    top_features = feature_analysis['most_active_features'][:5]\n",
    "    feature_indices = [f\"F{f['feature_idx']}\" for f in top_features]\n",
    "    activation_freqs = [f['activation_frequency'] for f in top_features]\n",
    "    \n",
    "    axes[1, 1].bar(feature_indices, activation_freqs)\n",
    "    axes[1, 1].set_title('Top 5 Most Active Features')\n",
    "    axes[1, 1].set_ylabel('Activation Frequency')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if we meet success criteria\n",
    "success = metrics['reconstruction_loss'] <= 0.15\n",
    "print(f\"\\n{'âœ… SUCCESS' if success else 'âŒ NEEDS IMPROVEMENT'}: Reconstruction loss {'meets' if success else 'exceeds'} target threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Discovered Features\n",
    "\n",
    "Now let's examine what our SAE has learned by looking at which tokens activate each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer for feature analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the trained SAE\n",
    "sae = SparseAutoencoder(\n",
    "    input_dim=metadata['input_dim'],\n",
    "    latent_dim=metadata['latent_dim'],\n",
    "    tied_weights=metadata['tied_weights']\n",
    ")\n",
    "sae.load_state_dict(sae_weights)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"Loaded SAE: {metadata['input_dim']} â†’ {metadata['latent_dim']}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature analyzer\n",
    "analyzer = FeatureAnalyzer(\n",
    "    sae_model=sae,\n",
    "    tokenizer=tokenizer,\n",
    "    activation_data_path=ACTIVATION_FILE,\n",
    "    layer_idx=LAYER_IDX\n",
    ")\n",
    "\n",
    "# Analyze a few interesting features\n",
    "interesting_features = [0, 1, 10, 50, 100]  # Sample some features\n",
    "\n",
    "print(\"Analyzing sample features...\")\n",
    "for feature_idx in interesting_features:\n",
    "    if feature_idx < sae.latent_dim:\n",
    "        print(f\"\\n=== FEATURE {feature_idx} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Get feature summary\n",
    "            summary = analyzer.get_feature_summary(feature_idx)\n",
    "            \n",
    "            print(f\"Weight norm: {summary['weight_norm']:.4f}\")\n",
    "            print(f\"Sparsity: {summary['sparsity']:.4f}\")\n",
    "            print(f\"Max activation: {summary['max_activation']:.4f}\")\n",
    "            print(f\"Top tokens: {summary['top_tokens']}\")\n",
    "            \n",
    "            # Show detailed token analysis\n",
    "            if len(summary['top_token_details']) > 0:\n",
    "                print(\"\\nTop activating contexts:\")\n",
    "                for i, token_info in enumerate(summary['top_token_details'][:3]):\n",
    "                    print(f\"  {i+1}. '{token_info['token']}' (strength: {token_info['activation_strength']:.3f})\")\n",
    "                    print(f\"     Context: ...{token_info['context_snippet']}...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing feature {feature_idx}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Feature Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoder weight patterns for some features\n",
    "encoder_weights = sae.encoder.weight.data.cpu().numpy()  # Shape: [latent_dim, input_dim]\n",
    "\n",
    "# Select a few features to visualize\n",
    "features_to_plot = [0, 1, 10, 50, 100]\n",
    "features_to_plot = [f for f in features_to_plot if f < encoder_weights.shape[0]]\n",
    "\n",
    "fig, axes = plt.subplots(len(features_to_plot), 1, figsize=(12, 3*len(features_to_plot)))\n",
    "if len(features_to_plot) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, feature_idx in enumerate(features_to_plot):\n",
    "    weights = encoder_weights[feature_idx]\n",
    "    \n",
    "    # Plot the weight vector\n",
    "    axes[i].plot(weights, alpha=0.7)\n",
    "    axes[i].set_title(f'Feature {feature_idx} Encoder Weights')\n",
    "    axes[i].set_xlabel('Input Dimension')\n",
    "    axes[i].set_ylabel('Weight Value')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    norm = np.linalg.norm(weights)\n",
    "    sparsity = (np.abs(weights) < 1e-6).mean()\n",
    "    axes[i].text(0.02, 0.98, f'Norm: {norm:.3f}\\nSparsity: {sparsity:.3f}', \n",
    "                transform=axes[i].transAxes, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Live LoRA Patching\n",
    "\n",
    "Now comes the exciting part - we'll create live patches to modify model behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full model for patching\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16)\n",
    "model.eval()\n",
    "\n",
    "# Create LoRA patcher\n",
    "patcher = LoRAPatcher(model)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Available for patching: {len(patcher._get_target_modules(LAYER_IDX, 'mlp'))} MLP modules\")\n",
    "print(f\"Available for patching: {len(patcher._get_target_modules(LAYER_IDX, 'attention'))} attention modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an interesting feature to patch\n",
    "feature_to_patch = 10  # Choose a feature that showed interesting tokens\n",
    "feature_vector = torch.tensor(encoder_weights[feature_to_patch], dtype=torch.float32)\n",
    "\n",
    "print(f\"Creating patch for feature {feature_to_patch}\")\n",
    "print(f\"Feature vector shape: {feature_vector.shape}\")\n",
    "print(f\"Feature vector norm: {feature_vector.norm():.4f}\")\n",
    "\n",
    "# Create a suppression patch (negative strength)\n",
    "patch_id = patcher.create_feature_patch(\n",
    "    feature_id=f\"feature_{feature_to_patch}\",\n",
    "    layer_idx=LAYER_IDX,\n",
    "    feature_vector=feature_vector,\n",
    "    strength=-1.0,  # Suppress this feature\n",
    "    rank=8,\n",
    "    target_type=\"mlp\"\n",
    ")\n",
    "\n",
    "print(f\"Created patch: {patch_id}\")\n",
    "print(f\"Active patches: {list(patcher.patch_metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the patch effect on some sample text\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"Machine learning is a fascinating field of study.\",\n",
    "    \"The weather is beautiful outside.\"\n",
    "]\n",
    "\n",
    "print(\"Testing patch effects...\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"=== Test {i+1}: {text} ===\")\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get original activations\n",
    "        patcher.disable_patch(patch_id)\n",
    "        original_outputs = model(**inputs)\n",
    "        original_hidden = original_outputs.last_hidden_state\n",
    "        \n",
    "        # Get patched activations\n",
    "        patcher.enable_patch(patch_id)\n",
    "        patched_outputs = model(**inputs)\n",
    "        patched_hidden = patched_outputs.last_hidden_state\n",
    "        \n",
    "        # Compare outputs\n",
    "        diff = torch.norm(patched_hidden - original_hidden).item()\n",
    "        original_norm = torch.norm(original_hidden).item()\n",
    "        relative_change = diff / original_norm * 100\n",
    "        \n",
    "        print(f\"  Original norm: {original_norm:.4f}\")\n",
    "        print(f\"  Difference norm: {diff:.4f}\")\n",
    "        print(f\"  Relative change: {relative_change:.2f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure patch latency (Success Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test patch toggle latency (target: <400ms)\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "latencies = []\n",
    "num_tests = 10\n",
    "\n",
    "print(f\"Measuring patch toggle latency ({num_tests} trials)...\")\n",
    "\n",
    "for i in range(num_tests):\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Toggle patch and run inference\n",
    "    patcher.enable_patch(patch_id)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    latencies.append(latency_ms)\n",
    "\n",
    "# Analyze latencies\n",
    "mean_latency = np.mean(latencies)\n",
    "std_latency = np.std(latencies)\n",
    "max_latency = np.max(latencies)\n",
    "\n",
    "print(f\"\\nLatency Results:\")\n",
    "print(f\"  Mean: {mean_latency:.1f} ms\")\n",
    "print(f\"  Std: {std_latency:.1f} ms\")\n",
    "print(f\"  Max: {max_latency:.1f} ms\")\n",
    "print(f\"  Target: <400 ms\")\n",
    "\n",
    "success = max_latency < 400\n",
    "print(f\"\\n{'âœ… SUCCESS' if success else 'âŒ NEEDS IMPROVEMENT'}: Latency {'meets' if success else 'exceeds'} target threshold\")\n",
    "\n",
    "# Plot latency distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(latencies, bins=max(5, num_tests//2), alpha=0.7, edgecolor='black')\n",
    "plt.axvline(400, color='red', linestyle='--', label='Target (400ms)')\n",
    "plt.axvline(mean_latency, color='green', linestyle='-', label=f'Mean ({mean_latency:.1f}ms)')\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Patch Toggle + Inference Latency Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Results\n",
    "\n",
    "Finally, let's export our trained SAE and patches for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export directory\n",
    "export_dir = Path(\"tutorial_exports\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Exporting results to {export_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SAE weights and metadata\n",
    "import shutil\n",
    "\n",
    "# Copy SAE files\n",
    "sae_export_path = export_dir / \"sae\"\n",
    "sae_export_path.mkdir(exist_ok=True)\n",
    "\n",
    "shutil.copy2(sae_path, sae_export_path / f\"sae_layer_{LAYER_IDX}.safetensors\")\n",
    "shutil.copy2(metadata_path, sae_export_path / f\"sae_layer_{LAYER_IDX}_metadata.json\")\n",
    "\n",
    "print(f\"âœ… SAE exported to {sae_export_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export patches\n",
    "patch_export_path = export_dir / \"patches\"\n",
    "patcher.save_patches(str(patch_export_path))\n",
    "\n",
    "print(f\"âœ… Patches exported to {patch_export_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export evaluation report\n",
    "report_export_path = export_dir / \"evaluation_report.json\"\n",
    "shutil.copy2(\"tutorial_eval_report.json\", report_export_path)\n",
    "\n",
    "print(f\"âœ… Evaluation report exported to {report_export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    \"tutorial_info\": {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"layer_idx\": LAYER_IDX,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "        \"latent_dim\": LATENT_DIM,\n",
    "        \"sparsity_coef\": SPARSITY_COEF\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"reconstruction_loss\": metrics['reconstruction_loss'],\n",
    "        \"explained_variance\": metrics['explained_variance'],\n",
    "        \"meets_loss_target\": metrics['reconstruction_loss'] <= 0.15,\n",
    "        \"mean_patch_latency_ms\": mean_latency,\n",
    "        \"meets_latency_target\": max_latency < 400,\n",
    "        \"dead_features\": feature_analysis['total_dead_features'],\n",
    "        \"total_features\": metrics['num_features']\n",
    "    },\n",
    "    \"files_created\": {\n",
    "        \"activations\": ACTIVATION_FILE,\n",
    "        \"sae_weights\": str(sae_path),\n",
    "        \"sae_metadata\": str(metadata_path),\n",
    "        \"evaluation_report\": \"tutorial_eval_report.json\",\n",
    "        \"exports\": str(export_dir)\n",
    "    }\n",
    "}\n",
    "\n",
    "summary_path = export_dir / \"tutorial_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Tutorial summary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've successfully completed the InterpretabilityWorkbench tutorial.\n",
    "\n",
    "### What you accomplished:\n",
    "\n",
    "1. âœ… **Recorded activations** from a language model using forward hooks\n",
    "2. âœ… **Trained a sparse autoencoder** to discover interpretable features\n",
    "3. âœ… **Evaluated SAE performance** against success criteria\n",
    "4. âœ… **Analyzed discovered features** to understand what tokens activate them\n",
    "5. âœ… **Created live LoRA patches** to modify model behavior in real-time\n",
    "6. âœ… **Measured latency** to verify <400ms patch toggle performance\n",
    "7. âœ… **Exported all results** for future use\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- **Scale up**: Try with larger models (Llama-2-7B, GPT-2-large)\n",
    "- **Explore features**: Use the web UI to interactively browse features\n",
    "- **Advanced patching**: Create patches targeting specific behaviors\n",
    "- **Multi-layer analysis**: Compare features across different layers\n",
    "- **Production deployment**: Use exported SAEs in your applications\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- ðŸ“š **Documentation**: See README.md for detailed API reference\n",
    "- ðŸŒ **Web UI**: Launch with `microscope ui` for interactive exploration\n",
    "- ðŸ”¬ **Advanced features**: Check out provenance graph analysis\n",
    "- ðŸš€ **Scale up**: Use the evaluation script for larger experiments\n",
    "\n",
    "Happy interpretability research! ðŸ”âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}